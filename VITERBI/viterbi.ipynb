{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CSE-143_A3_Code.ipynb","provenance":[{"file_id":"186hCS3cdEtl0vpkgCXHYgLUu-BkFZZ9T","timestamp":1583157228957}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"FyeebuhZdJb1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654302439350,"user_tz":420,"elapsed":1166,"user":{"displayName":"Shane Sawyer","userId":"15313489058341802227"}},"outputId":"17306173-a073-43d8-b04f-3abcad59d24e"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","import os\n","import sys\n","print(os.listdir('/content/gdrive/My Drive/CSE-143-A3'))\n","sys.path.append('/content/gdrive/My Drive/CSE-143-A3')"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n","['conlleval.py', 'ner.dev', 'model.simple', 'ner.test', 'ner.train', '__pycache__', 'model.sorted.txt', 'ner.test.out', 'ner.dev.out', 'CSE-143_A3_Code.ipynb']\n"]}]},{"cell_type":"code","source":["% cd '/content/gdrive/My Drive/CSE-143-A3'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C8w1fT2zNMvi","executionInfo":{"status":"ok","timestamp":1654302439350,"user_tz":420,"elapsed":5,"user":{"displayName":"Shane Sawyer","userId":"15313489058341802227"}},"outputId":"a12789ff-846d-4dba-8bcd-efa735474840"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/CSE-143-A3\n"]}]},{"cell_type":"code","metadata":{"id":"CHdbUbDpdTBp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654304608355,"user_tz":420,"elapsed":43623,"user":{"displayName":"Shane Sawyer","userId":"15313489058341802227"}},"outputId":"72a34ee7-9957-4574-ad95-de50301112b1"},"source":["import random\n","import math\n","from conlleval import evaluate as conllevaluate\n","\n","directory = '/content/gdrive/My Drive/CSE-143-A3'\n","\n","def decode(input_length, tagset, score):\n","    \"\"\"\n","    Compute the highest scoring sequence according to the scoring function.\n","    :param input_length: int. number of tokens in the input including <START> and <STOP>\n","    :param tagset: Array of strings, which are the possible tags.  Does not have <START>, <STOP>\n","    :param score: function from current_tag (string), previous_tag (string), i (int) to the score.  i=0 points to\n","        <START> and i=1 points to the first token. i=input_length-1 points to <STOP>\n","    :return: Array strings of length input_length, which is the highest scoring tag sequence including <START> and <STOP>\n","    \"\"\"\n","    # Look at the function compute_score for an example of how the tag sequence should be scored\n","    scores = []\n","    tags = []\n","\n","    for _ in range(input_length):\n","        scores.append(dict())\n","        tags.append(dict())\n","\n","    for i in range(0, input_length - 1):\n","        for t in tagset:\n","            if i < 1:\n","                scores[i][t] = score(t, \"<START>\", i)\n","            else:\n","                max = (-math.inf, \"\")\n","                for t2 in tagset:\n","                    s = score(t, t2, i) + scores[i-1][t2]\n","                    if s > max[0]:\n","                        max = (s, t2)\n","                scores[i][t], tags[i][t] = max\n","\n","    max = (-math.inf, \"\")\n","    for t in tagset:\n","        s = score(\"<STOP>\", t, input_length - 1) + scores[input_length - 2][t]\n","        if s > max[0]:\n","            max = (s, t)\n","\n","    seq = [\"<STOP>\", max[1]]\n","\n","    for i in range(1, input_length - 2):\n","        seq.append(tags[input_length - 1 - i][seq[-1]])\n","    seq.append(\"<START>\")\n","    return seq[::-1]\n","    \n","\n","def compute_score(tag_seq, input_length, score):\n","    \"\"\"\n","    Computes the total score of a tag sequence\n","    :param tag_seq: Array of String of length input_length. The tag sequence including <START> and <STOP>\n","    :param input_length: Int. input length including the padding <START> and <STOP>\n","    :param score: function from current_tag (string), previous_tag (string), i (int) to the score.  i=0 points to\n","        <START> and i=1 points to the first token. i=input_length-1 points to <STOP>\n","    :return:\n","    \"\"\"\n","    total_score = 0\n","    for i in range(1, input_length):\n","        total_score += score(tag_seq[i], tag_seq[i - 1], i)\n","    return total_score\n","\n","\n","def compute_features(tag_seq, input_length, features):\n","    \"\"\"\n","    Compute f(xi, yi)\n","    :param tag_seq: [tags] already padded with <START> and <STOP>\n","    :param input_length: input length including the padding <START> and <STOP>\n","    :param features: func from token index to FeatureVector\n","    :return:\n","    \"\"\"\n","    feats = FeatureVector({})\n","    for i in range(1, input_length):\n","        feats.times_plus_equal(1, features.compute_features(tag_seq[i], tag_seq[i - 1], i))\n","    return feats\n","\n","\n","def sgd(training_size, epochs, gradient, parameters, training_observer):\n","    \"\"\"\n","    Stochastic gradient descent\n","    :param training_size: int. Number of examples in the training set\n","    :param epochs: int. Number of epochs to run SGD for\n","    :param gradient: func from index (int) in range(training_size) to a FeatureVector of the gradient\n","    :param parameters: FeatureVector.  Initial parameters.  Should be updated while training\n","    :param training_observer: func that takes epoch and parameters.  You can call this function at the end of each\n","           epoch to evaluate on a dev set and write out the model parameters for early stopping.\n","    :return: final parameters\n","    \"\"\"\n","    # Look at the FeatureVector object.  You'll want to use the function times_plus_equal to update the\n","    # parameters.\n","    # To implement early stopping you can call the function training_observer at the end of each epoch.\n","    i = 0\n","\n","    while i < epochs:\n","        print('i='+str(i))\n","        data_indices = [ i for i in range(training_size) ]\n","        random.shuffle(data_indices)\n","        counter = 0\n","        for t in data_indices:\n","            if counter % 1000 == 0:\n","                print('Item '+str(counter))\n","            parameters.times_plus_equal(-1, gradient(t))\n","            counter += 1\n","        i += 1\n","        training_observer(i, parameters)\n","    return\n","\n","\n","def train(data, feature_names, tagset, epochs):\n","    \"\"\"\n","    Trains the model on the data and returns the parameters\n","    :param data: Array of dictionaries representing the data.  One dictionary for each data point (as created by the\n","        make_data_point function).\n","    :param feature_names: Array of Strings.  The list of feature names.\n","    :param tagset: Array of Strings.  The list of tags.\n","    :param epochs: Int. The number of epochs to train\n","    :return: FeatureVector. The learned parameters.\n","    \"\"\"\n","    parameters = FeatureVector({})\n","\n","    def perceptron_gradient(i):\n","        \"\"\"\n","        Computes the gradient of the Perceptron loss for example i\n","        :param i: Int\n","        :return: FeatureVector\n","        \"\"\"\n","        inputs = data[i]\n","        input_len = len(inputs['tokens'])\n","        gold_labels = inputs['gold_tags']\n","        features = Features(inputs, feature_names)\n","\n","        def score(cur_tag, pre_tag, i):\n","            return parameters.dot_product(features.compute_features(cur_tag, pre_tag, i))\n","\n","        tags = decode(input_len, tagset, score)\n","        fvector = compute_features(tags, input_len, features)           # Add the predicted features\n","        #print('Input:', inputs)        # helpful for debugging\n","        #print(\"Predicted Feature Vector:\", fvector.fdict)\n","        #print(\"Predicted Score:\", parameters.dot_product(fvector))\n","        fvector.times_plus_equal(-1, compute_features(gold_labels, input_len, features))    # Subtract the features for the gold labels\n","        #print(\"Gold Labels Feature Vector: \", compute_features(gold_labels, input_len, features).fdict)\n","        #print(\"Gold Labels Score:\", parameters.dot_product(compute_features(gold_labels, input_len, features)))\n","        return fvector\n","\n","    def training_observer(epoch, parameters):\n","        \"\"\"\n","        Evaluates the parameters on the development data, and writes out the parameters to a 'model.iter'+epoch and\n","        the predictions to 'ner.dev.out'+epoch.\n","        :param epoch: int.  The epoch\n","        :param parameters: Feature Vector.  The current parameters\n","        :return: Double. F1 on the development data\n","        \"\"\"\n","        dev_data = read_data('ner.dev')\n","        (_, _, f1) = evaluate(dev_data, parameters, feature_names, tagset)\n","        write_predictions('ner.dev.out'+str(epoch), dev_data, parameters, feature_names, tagset)\n","        parameters.write_to_file('model.iter'+str(epoch))\n","        return f1\n","\n","    return sgd(len(data), epochs, perceptron_gradient, parameters, training_observer)\n","\n","\n","def predict(inputs, input_len, parameters, feature_names, tagset):\n","    \"\"\"\n","    \n","    :param inputs:\n","    :param input_len:\n","    :param parameters:\n","    :param feature_names:\n","    :param tagset:\n","    :return:\n","    \"\"\"\n","    features = Features(inputs, feature_names)\n","\n","    def score(cur_tag, pre_tag, i):\n","        return parameters.dot_product(features.compute_features(cur_tag, pre_tag, i))\n","\n","    return decode(input_len, tagset, score)\n","\n","\n","def make_data_point(sent):\n","    \"\"\"\n","        Creates a dictionary from String to an Array of Strings representing the data.  The dictionary items are:\n","        dic['tokens'] = Tokens padded with <START> and <STOP>\n","        dic['pos'] = POS tags padded with <START> and <STOP>\n","        dic['NP_chunk'] = Tags indicating noun phrase chunks, padded with <START> and <STOP>\n","        dic['gold_tags'] = The gold tags padded with <START> and <STOP>\n","    :param sent: String.  The input CoNLL format string\n","    :return: Dict from String to Array of Strings.\n","    \"\"\"\n","    dic = {}\n","    sent = [s.strip().split() for s in sent]\n","    dic['tokens'] = ['<START>'] + [s[0] for s in sent] + ['<STOP>']\n","    dic['pos'] = ['<START>'] + [s[1] for s in sent] + ['<STOP>']\n","    dic['NP_chunk'] = ['<START>'] + [s[2] for s in sent] + ['<STOP>']\n","    dic['gold_tags'] = ['<START>'] + [s[3] for s in sent] + ['<STOP>']\n","    return dic\n","\n","def read_data(filename):\n","    \"\"\"\n","    Reads the CoNLL 2003 data into an array of dictionaries (a dictionary for each data point).\n","    :param filename: String\n","    :return: Array of dictionaries.  Each dictionary has the format returned by the make_data_point function.\n","    \"\"\"\n","    data = []\n","    with open(filename, 'r') as f:\n","        sent = []\n","        for line in f.readlines():\n","            if line.strip():\n","                sent.append(line)\n","            else:\n","                data.append(make_data_point(sent))\n","                sent = []\n","        data.append(make_data_point(sent))\n","\n","    return data\n","\n","def write_predictions(out_filename, all_inputs, parameters, feature_names, tagset):\n","    \"\"\"\n","    Writes the predictions on all_inputs to out_filename, in CoNLL 2003 evaluation format.\n","    Each line is token, pos, NP_chuck_tag, gold_tag, predicted_tag (separated by spaces)\n","    Sentences are separated by a newline\n","    The file can be evaluated using the command: python conlleval.py < out_file\n","    :param out_filename: filename of the output\n","    :param all_inputs:\n","    :param parameters:\n","    :param feature_names:\n","    :param tagset:\n","    :return:\n","    \"\"\"\n","    with open(out_filename, 'w', encoding='utf-8') as f:\n","        for inputs in all_inputs:\n","            input_len = len(inputs['tokens'])\n","            tag_seq = predict(inputs, input_len, parameters, feature_names, tagset)\n","            for i, tag in enumerate(tag_seq[1:-1]):  # deletes <START> and <STOP>\n","                f.write(' '.join([inputs['tokens'][i+1], inputs['pos'][i+1], inputs['NP_chunk'][i+1], inputs['gold_tags'][i+1], tag])+'\\n') # i + 1 because of <START>\n","            f.write('\\n')\n","\n","def evaluate(data, parameters, feature_names, tagset):\n","    \"\"\"\n","    Evaluates precision, recall, and F1 of the tagger compared to the gold standard in the data\n","    :param data: Array of dictionaries representing the data.  One dictionary for each data point (as created by the\n","        make_data_point function)\n","    :param parameters: FeatureVector.  The model parameters\n","    :param feature_names: Array of Strings.  The list of features.\n","    :param tagset: Array of Strings.  The list of tags.\n","    :return: Tuple of (prec, rec, f1)\n","    \"\"\"\n","    all_gold_tags = [ ]\n","    all_predicted_tags = [ ]\n","    for inputs in data:\n","        all_gold_tags.extend(inputs['gold_tags'][1:-1])  # deletes <START> and <STOP>\n","        input_len = len(inputs['tokens'])\n","        all_predicted_tags.extend(predict(inputs, input_len, parameters, feature_names, tagset)[1:-1]) # deletes <START> and <STOP>\n","    return conllevaluate(all_gold_tags, all_predicted_tags)\n","\n","\n","def test_decoder():\n","    # See \n","    \n","    tagset = ['NN', 'VB']     # make up our own tagset\n","\n","    def score_wrap(cur_tag, pre_tag, i):\n","        retval = score(cur_tag, pre_tag, i)\n","        print('Score('+cur_tag+','+pre_tag+','+str(i)+') returning '+str(retval))\n","        return retval\n","\n","    def score(cur_tag, pre_tag, i):\n","        if i == 0:\n","            print(\"ERROR: Don't call score for i = 0 (that points to <START>, with nothing before it)\")\n","        if i == 1:\n","            if pre_tag != '<START>':\n","                print(\"ERROR: Previous tag should be <START> for i = 1. Previous tag = \"+pre_tag)\n","            if cur_tag == 'NN':\n","                return 6\n","            if cur_tag == 'VB':\n","                return 4\n","        if i == 2:\n","            if cur_tag == 'NN' and pre_tag == 'NN':\n","                return 4\n","            if cur_tag == 'NN' and pre_tag == 'VB':\n","                return 9\n","            if cur_tag == 'VB' and pre_tag == 'NN':\n","                return 5\n","            if cur_tag == 'VB' and pre_tag == 'VB':\n","                return 0\n","        if i == 3:\n","            if cur_tag != '<STOP>':\n","                print('ERROR: Current tag at i = 3 should be <STOP>. Current tag = '+cur_tag)\n","            if pre_tag == 'NN':\n","                return 1\n","            if pre_tag == 'VB':\n","                return 1\n","\n","    predicted_tag_seq = decode(4, tagset, score_wrap)\n","    print('Predicted tag sequence should be = <START> VB NN <STOP>')\n","    print('Predicted tag sequence = '+' '.join(predicted_tag_seq))\n","    print(\"Score of ['<START>','VB','NN','<STOP>'] = \"+str(compute_score(['<START>','VB','NN','<STOP>'], 4, score)))\n","    print('Max score should be = 14')\n","    print('Max score = '+str(compute_score(predicted_tag_seq, 4, score)))\n","\n","\n","def main_predict(data_filename, model_filename):\n","    \"\"\"\n","    Main function to make predictions.\n","    Loads the model file and runs the NER tagger on the data, writing the output in CoNLL 2003 evaluation format to data_filename.out\n","    :param data_filename: String\n","    :param model_filename: String\n","    :return: None\n","    \"\"\"\n","    data = read_data(data_filename)\n","    parameters = FeatureVector({})\n","    parameters.read_from_file(model_filename)\n","\n","    tagset = ['B-PER', 'B-LOC', 'B-ORG', 'B-MISC', 'I-PER', 'I-LOC', 'I-ORG', 'I-MISC', 'O']\n","    feature_names = ['tag', 'prev_tag', 'current_word']\n","\n","    write_predictions(data_filename+'.out', data, parameters, feature_names, tagset)\n","    evaluate(data, parameters, feature_names, tagset)\n","\n","    return\n","\n","\n","def main_train():\n","    \"\"\"\n","    Main function to train the model\n","    :return: None\n","    \"\"\"\n","    print('Reading training data')\n","    train_data = read_data('ner.train')\n","    #train_data = read_data('ner.train')[1:1] # if you want to train on just one example\n","\n","    tagset = ['B-PER', 'B-LOC', 'B-ORG', 'B-MISC', 'I-PER', 'I-LOC', 'I-ORG', 'I-MISC', 'O']\n","    feature_names = ['tag', 'prev_tag', 'current_word']\n","\n","    print('Training...')\n","    parameters = train(train_data, feature_names, tagset, epochs=10)\n","    print('Training done')\n","    dev_data = read_data('ner.dev')\n","    evaluate(dev_data, parameters, feature_names, tagset)\n","    test_data = read_data('ner.test')\n","    evaluate(test_data, parameters, feature_names, tagset)\n","    parameters.write_to_file('model')\n","\n","    return\n","\n","\n","class Features(object):\n","    def __init__(self, inputs, feature_names):\n","        \"\"\"\n","        Creates a Features object\n","        :param inputs: Dictionary from String to an Array of Strings.\n","            Created in the make_data_point function.\n","            inputs['tokens'] = Tokens padded with <START> and <STOP>\n","            inputs['pos'] = POS tags padded with <START> and <STOP>\n","            inputs['NP_chunk'] = Tags indicating noun phrase chunks, padded with <START> and <STOP>\n","            inputs['gold_tags'] = DON'T USE! The gold tags padded with <START> and <STOP>\n","        :param feature_names: Array of Strings.  The list of features to compute.\n","        \"\"\"\n","        self.feature_names = feature_names\n","        self.inputs = inputs\n","\n","    def compute_features(self, cur_tag, pre_tag, i):\n","        \"\"\"\n","        Computes the local features for the current tag, the previous tag, and position i\n","        :param cur_tag: String.  The current tag.\n","        :param pre_tag: String.  The previous tag.\n","        :param i: Int. The position\n","        :return: FeatureVector\n","        \"\"\"\n","        feats = FeatureVector({})\n","        if 'tag' in self.feature_names:\n","            feats.times_plus_equal(1, FeatureVector({'t='+cur_tag: 1}))\n","        if 'prev_tag' in self.feature_names:\n","            feats.times_plus_equal(1, FeatureVector({'ti='+cur_tag+\"+ti-1=\"+pre_tag: 1}))\n","        if 'current_word' in self.feature_names:\n","            feats.times_plus_equal(1, FeatureVector({'t='+cur_tag+'+w='+self.inputs['tokens'][i]: 1}))\n","        return feats\n","\n","class FeatureVector(object):\n","\n","    def __init__(self, fdict):\n","        self.fdict = fdict\n","\n","    def times_plus_equal(self, scalar, v2):\n","        \"\"\"\n","        self += scalar * v2\n","        :param scalar: Double\n","        :param v2: FeatureVector\n","        :return: None\n","        \"\"\"\n","        for key, value in v2.fdict.items():\n","            self.fdict[key] = scalar * value + self.fdict.get(key, 0)\n","\n","\n","    def dot_product(self, v2):\n","        \"\"\"\n","        Computes the dot product between self and v2.  It is more efficient for v2 to be the smaller vector (fewer\n","        non-zero entries).\n","        :param v2: FeatureVector\n","        :return: Int\n","        \"\"\"\n","        retval = 0\n","        for key, value in v2.fdict.items():\n","            retval += value * self.fdict.get(key, 0)\n","        return retval\n","\n","    def write_to_file(self, filename):\n","        \"\"\"\n","        Writes the feature vector to a file.\n","        :param filename: String\n","        :return: None\n","        \"\"\"\n","        print('Writing to ' + filename)\n","        with open(filename, 'w', encoding='utf-8') as f:\n","            for key, value in self.fdict.items():\n","                f.write('{} {}\\n'.format(key, value))\n","\n","\n","    def read_from_file(self, filename):\n","        \"\"\"\n","        Reads a feature vector from a file.\n","        :param filename: String\n","        :return: None\n","        \"\"\"\n","        self.fdict = {}\n","        with open(filename, 'r') as f:\n","            for line in f.readlines():\n","                txt = line.split()\n","                self.fdict[txt[0]] = float(txt[1])\n","\n","#test_decoder()  # Uncomment to test the decoder on a simple example (see https://classes.soe.ucsc.edu/cse143/Winter20/assignments/A3_Debug_Example.pdf)\n","main_predict('ner.dev', 'model.iter7')  # Uncomment to predict on 'dev.ner' using the model 'model.simple' (need to implement 'decode' function)\n","# main_train()    # Uncomment to train a model (need to implement 'sgd' function)\n","\n"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["processed 51578 tokens with 5917 phrases; found: 5065 phrases; correct: 4271.\n","accuracy:  74.16%; (non-O)\n","accuracy:  95.32%; precision:  84.32%; recall:  72.18%; FB1:  77.78\n","              LOC: precision:  87.51%; recall:  83.11%; FB1:  85.26  1738\n","             MISC: precision:  78.89%; recall:  76.48%; FB1:  77.67  886\n","              ORG: precision:  78.57%; recall:  62.34%; FB1:  69.52  1064\n","              PER: precision:  88.24%; recall:  66.32%; FB1:  75.72  1377\n"]}]},{"cell_type":"markdown","metadata":{"id":"oInyKo1pfTW2"},"source":["To sort the model weights for easy viewing, you can use Unix commands:"]},{"cell_type":"code","metadata":{"id":"gHEtAu_LfmPG","executionInfo":{"status":"ok","timestamp":1654304742484,"user_tz":420,"elapsed":601,"user":{"displayName":"Shane Sawyer","userId":"15313489058341802227"}}},"source":["!cat \"/content/gdrive/My Drive/CSE-143-A3/model.iter7\" | awk '{print $2, $1}' | sort -gr > \"/content/gdrive/My Drive/CSE-143-A3/model.sorted.txt\""],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2bS4d4Acfp1S"},"source":["The file `model.sorted.txt` will be viewable in your Google Drive folder."]}]}